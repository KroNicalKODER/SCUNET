{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lsReJDf50Dn",
        "outputId": "14b89717-a0ca-4a03-f3e2-6534b737cd74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "05cfMmJh6qNi",
        "outputId": "67179400-4f2d-455a-cde8-f2384436f948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HELPER FUNCITONS"
      ],
      "metadata": {
        "id": "men8F4VMKS7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCTIONS\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define color ranges for each class\n",
        "# List of RGB ranges corresponding to each class index\n",
        "idx_to_color = [\n",
        "    [(120, 120, 120), (135, 135, 135)],  # 0: Road (approximate range for [128, 128, 128])\n",
        "    [(120, 0, 0), (135, 15, 15)],        # 1: Sidewalk (approximate range for [128, 0, 0])\n",
        "    [(185, 185, 120), (200, 200, 135)],  # 2: Building (approximate range for [192, 192, 128])\n",
        "    [(120, 55, 120), (135, 75, 135)],    # 3: Vegetation (approximate range for [128, 64, 128])\n",
        "    [(50, 30, 215), (70, 50, 230)],      # 4: Sky (approximate range for [60, 40, 222])\n",
        "    [(120, 120, 0), (135, 135, 15)],     # 5: Traffic Sign (approximate range for [128, 128, 0])\n",
        "    [(185, 120, 120), (200, 135, 135)],  # 6: Pedestrian (approximate range for [192, 128, 128])\n",
        "    [(55, 55, 120), (75, 75, 135)],      # 7: Vehicle (approximate range for [64, 64, 128])\n",
        "    [(55, 0, 120), (75, 15, 135)],       # 8: Pole (approximate range for [64, 0, 128])\n",
        "    [(55, 55, 0), (75, 75, 15)],         # 9: Fence (approximate range for [64, 64, 0])\n",
        "    [(0, 120, 185), (15, 135, 200)]      # 10: Road Marking (approximate range for [0, 128, 192])\n",
        "]\n",
        "\n",
        "# Mapping of class indices to category IDs\n",
        "name_to_category = {\n",
        "    0: 0,   # Road\n",
        "    1: 1,   # Sidewalk\n",
        "    2: 2,   # Building\n",
        "    3: 3,   # Vegetation\n",
        "    4: 4,   # Sky\n",
        "    5: 5,   # Traffic Sign\n",
        "    6: 6,   # Pedestrian\n",
        "    7: 7,   # Vehicle\n",
        "    8: 8,   # Pole\n",
        "    9: 9,   # Fence\n",
        "    10: 10  # Road Marking\n",
        "}\n",
        "\n",
        "def one_hot_to_rgb(one_hot_mask, idx_to_color):\n",
        "    \"\"\"Convert batch of one-hot encoded masks back to RGB format.\"\"\"\n",
        "    # Check if there's a batch dimension\n",
        "    if one_hot_mask.dim() == 4:\n",
        "        batch_size, num_classes, height, width = one_hot_mask.shape\n",
        "        rgb_batch = []\n",
        "\n",
        "        # Process each mask in the batch\n",
        "        for i in range(batch_size):\n",
        "            class_indices = torch.argmax(one_hot_mask[i], dim=0).cpu().numpy()  # (H, W)\n",
        "            rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "            # Map each class index to its corresponding RGB color\n",
        "            for class_idx, color_range in enumerate(idx_to_color):\n",
        "                rgb_color = color_range[0]  # Use the first color in the range\n",
        "                rgb_mask[class_indices == class_idx] = rgb_color  # Assign RGB color to the appropriate pixels\n",
        "\n",
        "            # Convert to tensor and permute to match (C, H, W) format\n",
        "            rgb_mask = torch.from_numpy(rgb_mask).permute(2, 0, 1).float() / 255.0\n",
        "            rgb_batch.append(rgb_mask)\n",
        "\n",
        "        # Stack the batch back together\n",
        "        rgb_batch = torch.stack(rgb_batch).to(one_hot_mask.device)\n",
        "        return rgb_batch\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Expected input with batch dimension [B, C, H, W]\")\n",
        "\n",
        "\n",
        "\n",
        "def canny_edge_extraction(mask):\n",
        "    \"\"\"Extract edges from the mask using Canny edge detection.\"\"\"\n",
        "    gray_mask = cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray_mask, threshold1=100, threshold2=200)\n",
        "    edges = np.expand_dims(edges, axis=0)  # Add channel dimension\n",
        "    return torch.tensor(edges, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "def rgb_to_class_idx(mask):\n",
        "    \"\"\"Convert RGB mask to class indices based on color ranges.\"\"\"\n",
        "    mask = np.array(mask)\n",
        "    class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int32)\n",
        "    for class_idx, (min_color, max_color) in enumerate(idx_to_color):\n",
        "        mask_range = (mask >= min_color) & (mask <= max_color)\n",
        "        class_mask[mask_range.all(axis=2)] = class_idx\n",
        "    return torch.tensor(class_mask, dtype=torch.long)\n",
        "\n",
        "def one_hot_encode(mask, num_classes):\n",
        "    \"\"\"Convert class index mask to one-hot encoded format.\"\"\"\n",
        "    return torch.nn.functional.one_hot(mask, num_classes=num_classes).permute(2, 0, 1).float()\n",
        "\n",
        "def preprocess_mask(mask, num_classes=11):\n",
        "    \"\"\"Preprocess mask by converting to class indices, one-hot encoding, and adding Canny edges.\"\"\"\n",
        "    class_mask = rgb_to_class_idx(mask)\n",
        "    one_hot_mask = one_hot_encode(class_mask, num_classes)\n",
        "    edges = canny_edge_extraction(mask)\n",
        "    combined_mask = torch.cat([one_hot_mask, edges], dim=0)  # Concatenate along channel dimension\n",
        "    return combined_mask\n"
      ],
      "metadata": {
        "id": "jf0ESSkV57I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAKING THE CUSTOM DATASET"
      ],
      "metadata": {
        "id": "rbpW3BIgKWvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DATASET\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, num_classes=11):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_files = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert mask from RGB to class indices and one-hot encode it\n",
        "        class_mask = rgb_to_class_idx(mask)\n",
        "        one_hot_mask = one_hot_encode(class_mask, self.num_classes)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, one_hot_mask\n"
      ],
      "metadata": {
        "id": "6f2pzpaL6oEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL DEFINITION"
      ],
      "metadata": {
        "id": "mcaq3Iv1Kb1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DEFINING THE MODEL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Embed patches for input to Swin Transformer.\"\"\"\n",
        "    def __init__(self, in_channels, embed_dim, patch_size=4):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.patch_embed(x)\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"Single Swin Transformer Block with Window and Shifted Window Self-Attention.\"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(dim)\n",
        "        self.msa = nn.MultiheadAttention(dim, num_heads)\n",
        "        self.ln2 = nn.LayerNorm(dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape x to (batch_size * height * width, channels) for LayerNorm and MSA\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        x = x.permute(0, 2, 3, 1).contiguous().view(-1, channels)\n",
        "\n",
        "        # Apply LayerNorm and MSA\n",
        "        x_ln = self.ln1(x)\n",
        "        x_msa, _ = self.msa(x_ln.unsqueeze(0), x_ln.unsqueeze(0), x_ln.unsqueeze(0))\n",
        "        x = x + x_msa.squeeze(0)\n",
        "\n",
        "        # Apply FFN\n",
        "        x_ln = self.ln2(x)\n",
        "        x_ffn = self.ffn(x_ln)\n",
        "        x = x + x_ffn\n",
        "\n",
        "        # Reshape back to (batch_size, channels, height, width)\n",
        "        x = x.view(batch_size, height, width, channels).permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder based on Swin Transformer.\"\"\"\n",
        "    def __init__(self, in_channels, embed_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.patch_embed = PatchEmbedding(in_channels, embed_dim)\n",
        "        self.swin_transformers = nn.ModuleList([\n",
        "            SwinTransformerBlock(embed_dim, num_heads=3),\n",
        "            SwinTransformerBlock(embed_dim, num_heads=3)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        for transformer in self.swin_transformers:\n",
        "            x = transformer(x)\n",
        "        return x\n",
        "\n",
        "class CRFBlock(nn.Module):\n",
        "    \"\"\"Conditional Residual Fusion block with WAM and OLM.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CRFBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "        self.olm = OppositionLearningMechanism(out_channels)\n",
        "        self.wam = WeightAssignmentMechanism(out_channels)\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "        x = self.conv1(x)\n",
        "        x = self.lrelu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.wam(x)\n",
        "        x = self.olm(x, condition)  # The OLM will adjust both channel and spatial dimensions\n",
        "        return x\n",
        "\n",
        "\n",
        "class OppositionLearningMechanism(nn.Module):\n",
        "    \"\"\"Opposition-based Learning Mechanism for enhancing semantic feature information.\"\"\"\n",
        "    def __init__(self, x_channels):\n",
        "        super(OppositionLearningMechanism, self).__init__()\n",
        "        # Adjust condition to have the same number of channels as x\n",
        "        self.channel_adjust = nn.Conv2d(in_channels=x_channels, out_channels=x_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "        # Match channel dimensions of condition to x\n",
        "        condition = self.channel_adjust(condition)\n",
        "        # Match spatial dimensions of condition to x\n",
        "        condition = F.interpolate(condition, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=True)\n",
        "        return x * (1 - condition) + condition\n",
        "\n",
        "class WeightAssignmentMechanism(nn.Module):\n",
        "    \"\"\"Assign attention weights on channel and spatial dimensions.\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(WeightAssignmentMechanism, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.pool(self.conv(x))\n",
        "        return x * self.sigmoid(w)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder with CRF and Swin Transformer.\"\"\"\n",
        "    def __init__(self, embed_dim, out_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.crf_block = CRFBlock(embed_dim, embed_dim)\n",
        "        self.swin_transformers = nn.ModuleList([\n",
        "            SwinTransformerBlock(embed_dim, num_heads=3),\n",
        "            SwinTransformerBlock(embed_dim, num_heads=3)\n",
        "        ])\n",
        "        self.final_conv = nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, low_level_feat):\n",
        "        x = self.crf_block(x, low_level_feat)\n",
        "        for transformer in self.swin_transformers:\n",
        "            x = transformer(x)\n",
        "        return torch.tanh(self.final_conv(x))\n",
        "\n",
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    \"\"\"Multi-Scale Discriminator with PatchGAN.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.discriminator_blocks = nn.ModuleList([\n",
        "            PatchGANDiscriminator(in_channels),\n",
        "            PatchGANDiscriminator(in_channels)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.mean(torch.stack([d(x) for d in self.discriminator_blocks], dim=0))\n",
        "\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    \"\"\"Single PatchGAN Discriminator.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(PatchGANDiscriminator, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class SC_UNet(nn.Module):\n",
        "    \"\"\"Complete SC-UNet model combining Encoder, Decoder and Discriminator for GAN training.\"\"\"\n",
        "    def __init__(self, in_channels, embed_dim, out_channels):\n",
        "        super(SC_UNet, self).__init__()\n",
        "        self.encoder = Encoder(in_channels, embed_dim)\n",
        "        self.decoder = Decoder(embed_dim, out_channels)\n",
        "        self.discriminator = MultiScaleDiscriminator(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_features = self.encoder(x)  # Use encoded features as low_level_feat\n",
        "        decoded_image = self.decoder(encoded_features, encoded_features)  # Pass encoded_features as low_level_feat\n",
        "        return decoded_image, self.discriminator(decoded_image)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = SC_UNet(in_channels=3, embed_dim=96, out_channels=3)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pBz_UC7C8hR8",
        "outputId": "eaec5410-f3a5-4ef6-c6a8-a3e393e6eaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SC_UNet(\n",
            "  (encoder): Encoder(\n",
            "    (patch_embed): PatchEmbedding(\n",
            "      (patch_embed): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
            "    )\n",
            "    (swin_transformers): ModuleList(\n",
            "      (0-1): 2 x SwinTransformerBlock(\n",
            "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (msa): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=96, out_features=96, bias=True)\n",
            "        )\n",
            "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (crf_block): CRFBlock(\n",
            "      (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "      (olm): OppositionLearningMechanism(\n",
            "        (channel_adjust): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (wam): WeightAssignmentMechanism(\n",
            "        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (swin_transformers): ModuleList(\n",
            "      (0-1): 2 x SwinTransformerBlock(\n",
            "        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (msa): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=96, out_features=96, bias=True)\n",
            "        )\n",
            "        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_conv): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (discriminator): MultiScaleDiscriminator(\n",
            "    (discriminator_blocks): ModuleList(\n",
            "      (0-1): 2 x PatchGANDiscriminator(\n",
            "        (layers): Sequential(\n",
            "          (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.2)\n",
            "          (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "          (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "          (4): LeakyReLU(negative_slope=0.2)\n",
            "          (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "          (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "          (7): LeakyReLU(negative_slope=0.2)\n",
            "          (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "          (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "          (10): LeakyReLU(negative_slope=0.2)\n",
            "          (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOSS AND OPTIMISE"
      ],
      "metadata": {
        "id": "1kw-ak_kKf6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LOSS FUNCTION AND OPTIMISER\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hinge_adversarial_loss(real, fake):\n",
        "    loss_real = torch.mean(F.relu(1.0 - real))\n",
        "    loss_fake = torch.mean(F.relu(1.0 + fake))\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "def feature_matching_loss(real_features, fake_features):\n",
        "    \"\"\"Calculate feature matching loss by converting real_features (masks) to RGB format.\"\"\"\n",
        "    # Convert real_features to RGB format to match fake_features\n",
        "    real_features_rgb = one_hot_to_rgb(real_features, idx_to_color)\n",
        "\n",
        "    # Ensure both tensors have the same spatial dimensions\n",
        "    if real_features_rgb.shape != fake_features.shape:\n",
        "        real_features_rgb = F.interpolate(real_features_rgb, size=fake_features.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    # Calculate mean absolute difference\n",
        "    loss = torch.mean(torch.abs(real_features_rgb - fake_features))\n",
        "    return loss\n",
        "\n",
        "def perceptual_loss(real, fake, feature_extractor):\n",
        "    real_features = feature_extractor(real)\n",
        "    fake_features = feature_extractor(fake)\n",
        "    return feature_matching_loss(real_features, fake_features)\n",
        "\n",
        "# Instantiate the optimizer\n",
        "generator_optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = optim.Adam(model.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "hoeTmsFZ7rZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ACCURACY AND VALIDATION"
      ],
      "metadata": {
        "id": "48zNTqhJKjnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_accuracy(predictions, targets):\n",
        "    \"\"\"Compute accuracy by comparing predictions to target classes.\"\"\"\n",
        "    # Ensure predictions and targets have the same spatial dimensions\n",
        "    if predictions.shape[2:] != targets.shape[2:]:\n",
        "        predictions = F.interpolate(predictions, size=targets.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    # Convert to class indices\n",
        "    predicted_classes = torch.argmax(predictions, dim=1)  # Get class with highest probability\n",
        "    target_classes = torch.argmax(targets, dim=1)         # Convert one-hot to class index\n",
        "\n",
        "    # Compute accuracy\n",
        "    correct = (predicted_classes == target_classes).float()\n",
        "    return correct.sum() / correct.numel()\n",
        "\n",
        "def validate(model, val_dataloader, device):\n",
        "    \"\"\"Evaluate model on the validation set.\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_images = model(images)\n",
        "            if isinstance(fake_images, tuple):\n",
        "                fake_images = fake_images[0]\n",
        "            print(fake_images.shape)\n",
        "\n",
        "            # Resize fake images to match masks' spatial dimensions\n",
        "            if fake_images.shape[2:] != masks.shape[2:]:\n",
        "                fake_images = F.interpolate(fake_images, size=masks.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            # Calculate MSE loss\n",
        "            loss = F.mse_loss(fake_images, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            val_accuracy += compute_accuracy(fake_images, masks).item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_accuracy /= len(val_dataloader)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for images, masks in train_dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Convert one-hot mask to RGB format for the discriminator\n",
        "            rgb_masks = one_hot_to_rgb(masks, idx_to_color)\n",
        "\n",
        "            # 1. Train the discriminator\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                fake_images, _ = model(images)\n",
        "            real_pred = model.discriminator(rgb_masks)  # Use RGB masks\n",
        "            fake_pred = model.discriminator(fake_images.detach())\n",
        "            d_loss = hinge_adversarial_loss(real_pred, fake_pred)\n",
        "            d_loss.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "            # 2. Train the generator\n",
        "            generator_optimizer.zero_grad()\n",
        "            fake_images, _ = model(images)\n",
        "            fake_pred = model.discriminator(fake_images)\n",
        "            g_loss_adv = -torch.mean(fake_pred)\n",
        "            g_loss_fm = feature_matching_loss(masks, fake_images)\n",
        "            g_loss = g_loss_adv + 10 * g_loss_fm  # Adjust weights based on experiments\n",
        "            g_loss.backward()\n",
        "            generator_optimizer.step()\n",
        "\n",
        "            # Update training loss and accuracy\n",
        "            train_loss += g_loss.item()\n",
        "            train_accuracy += compute_accuracy(fake_images, masks).item()\n",
        "\n",
        "        # Average training loss and accuracy for the epoch\n",
        "        train_loss /= len(train_dataloader)\n",
        "        train_accuracy /= len(train_dataloader)\n",
        "\n",
        "        # Validate the model\n",
        "        # val_loss, val_accuracy = validate(model, val_dataloader, device)\n",
        "\n",
        "        # Print metrics for this epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        # print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "MILuOqzD7tSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "edI03vSUKqaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load training dataset\n",
        "train_dataset = CityscapesDataset(image_dir='add_path_here', mask_dir='add_path_here', transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=46, shuffle=True)\n",
        "\n",
        "# Load validation dataset\n",
        "val_dataset = CityscapesDataset(image_dir='add_path_here', mask_dir='add_path_here', transform=transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=46, shuffle=False)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SC_UNet(in_channels=3, embed_dim=96, out_channels=3).to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVd1qS88dbz",
        "outputId": "3713c6a8-28aa-444c-8602-f586a0dbf66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training with validation\n",
        "train(model, train_dataloader, val_dataloader, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T4wpVWZyZI-",
        "outputId": "e4f28131-7ee7-426e-8da6-94a7e1b1da4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2]\n",
            "  Train Loss: 6.4544, Train Accuracy: 0.7044\n",
            "Epoch [2/2]\n",
            "  Train Loss: 6.4549, Train Accuracy: 0.7045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"add_path_here\"\n",
        "\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "torch.save(model, \"add_path_here\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yNMNGjwL0iZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USING THE MODEL"
      ],
      "metadata": {
        "id": "_mDp5We11GgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = SC_UNet(in_channels=11, embed_dim=96, out_channels=3).to(device)\n",
        "model.load_state_dict(torch.load(\"sc_unet_model.pth\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "AvTQBOpQ0s7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_mask(mask_path, idx_to_color, num_classes=11):\n",
        "    \"\"\"Load and preprocess the mask from RGB format to one-hot encoding.\"\"\"\n",
        "    # Load mask and convert to numpy array\n",
        "    mask = Image.open(mask_path).convert(\"RGB\")\n",
        "    mask_np = np.array(mask)\n",
        "\n",
        "    # Initialize one-hot encoded mask with shape (num_classes, H, W)\n",
        "    one_hot_mask = np.zeros((num_classes, mask_np.shape[0], mask_np.shape[1]), dtype=np.float32)\n",
        "\n",
        "    # Populate one-hot encoded mask\n",
        "    for class_idx, color_range in enumerate(idx_to_color):\n",
        "        color = color_range[0]  # Use the primary color for this class\n",
        "        match_pixels = np.all(mask_np == color, axis=-1)\n",
        "        one_hot_mask[class_idx][match_pixels] = 1\n",
        "\n",
        "    # Convert to torch tensor and add a batch dimension\n",
        "    one_hot_mask = torch.tensor(one_hot_mask).unsqueeze(0).to(device)\n",
        "    return one_hot_mask\n",
        "\n",
        "# Example usage\n",
        "one_hot_mask = preprocess_mask(\"path_to_mask_image.png\", idx_to_color)\n"
      ],
      "metadata": {
        "id": "S3tNk0zY1KBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    generated_image = model(one_hot_mask)\n",
        "    generated_image = (generated_image[0] * 255).clamp(0, 255).byte().cpu()\n",
        "    generated_image_pil = transforms.ToPILImage()(generated_image)\n",
        "\n",
        "generated_image_pil.show()"
      ],
      "metadata": {
        "id": "3frmrQJe1Rov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}